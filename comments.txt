Привет, Кирилл!
Прокомментировал твои замечания. Посмотри, пожалуйста.

1. "Импорты внутри with DAG не являются хорошей практикой и могут негативно повлиять на производительность. Импорты должны быть в начале файла"

Как понял из описания https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html 
импорт в начале файла может негативно сказаться на производительности.
Правильно понимаю, что ты предлагаешь все библиотеки импортировать на верхнем уровне DAG ?
В моей реализации импорт производится в момент выполнения DAG, а не на уровне планировщика.
Буду очень признателен если ты подробнее осветишь этот момент с сылками на источники.
Отдельно буду очень благодарен если ты приложишь пример теста для оценки фактических затрат ресурсов для каждого типа реализации.

2. "Это очень странная реализация, давай вынесем все скрипты в отдельные sql файлы и уже будем вызывать"

Такой подход выбрал для возможности работы с текстом запроса при помощи функций python. 
Динамический SQL как будто менее читабелен и понятен.

Можно конечно импортировать sql файлы как-то так:

import os
file_path = "sql.sql"

if os.path.exists(file_path):
    with open(file_path, "r") as file:
        sql_code = file.read()
else:
    print(f"File '{file_path}' does not exist.")

Но я не уверен, что этот способ проще и чем-то лучше.
Буду раз услышать от тебя совет по альтернативному получению текста запроса из файла с возможностью использования как шаблон.